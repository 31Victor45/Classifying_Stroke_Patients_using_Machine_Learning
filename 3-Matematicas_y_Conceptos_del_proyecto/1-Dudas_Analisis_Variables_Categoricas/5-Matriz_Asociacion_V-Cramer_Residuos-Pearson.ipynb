{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e1eff82",
   "metadata": {},
   "source": [
    "# Coeficiente V de Cramer (con Corrección de Sesgo)\n",
    "\n",
    "La **V de Cramer** es una medida de asociación para variables nominales. A diferencia del coeficiente de correlación de Pearson (que es para variables continuas), la V de Cramer nos permite entender qué tan relacionadas están dos variables cualitativas.\n",
    "\n",
    "### 1. Definición Matemática\n",
    "Para una tabla de contingencia de dimensiones $r \\times c$ (filas x columnas), la fórmula original es:\n",
    "\n",
    "$$V = \\sqrt{\\frac{\\phi^2}{\\min(k-1, r-1)}} = \\sqrt{\\frac{\\chi^2 / n}{\\min(k-1, r-1)}}$$\n",
    "\n",
    "Donde:\n",
    "* $\\chi^2$: Estadístico Chi-cuadrado de Pearson.\n",
    "* $n$: Tamaño total de la muestra.\n",
    "* $k, r$: Número de categorías de las variables.\n",
    "\n",
    "### 2. Corrección de Sesgo (Bias Correction)\n",
    "La V de Cramer original tiende a sobreestimar la asociación en muestras pequeñas o tablas con muchas categorías. La versión corregida (propuesta por Bergsma y Wicher en 2013) utiliza ajustes en $\\chi^2$ y en los grados de libertad:\n",
    "\n",
    "1. Se define $\\tilde{\\chi}^2 = \\max(0, \\chi^2 - \\frac{(r-1)(c-1)}{n-1})$\n",
    "2. Se definen $\\tilde{r} = r - \\frac{(r-1)^2}{n-1}$ y $\\tilde{c} = c - \\frac{(c-1)^2}{n-1}$\n",
    "3. La fórmula corregida es:\n",
    "$$\\tilde{V} = \\sqrt{\\frac{\\tilde{\\chi}^2 / n}{\\min(\\tilde{r}-1, \\tilde{c}-1)}}$$\n",
    "\n",
    "### 3. Interpretación de Valores\n",
    "* **0.0 - 0.1:** Asociación insignificante.\n",
    "* **0.1 - 0.3:** Asociación débil.\n",
    "* **0.3 - 0.5:** Asociación moderada.\n",
    "* **> 0.5:** Asociación fuerte.\n",
    "\n",
    "---\n",
    "\n",
    "## Usos en Machine Learning (ML)\n",
    "\n",
    "La matriz de asociación de V de Cramer es fundamental en las etapas iniciales del pipeline de ML:\n",
    "\n",
    "1. **Análisis Exploratorio de Datos (EDA):** Permite detectar visualmente (mediante un Heatmap) qué variables categóricas están \"conectadas\" entre sí.\n",
    "2. **Selección de Características (Feature Selection):** * Ayuda a identificar variables predictoras altamente relacionadas con el *target* categórico.\n",
    "    * Permite eliminar variables redundantes para reducir la dimensionalidad.\n",
    "3. **Detección de Multicolinealidad:** Identifica si dos variables de entrada aportan la misma información, lo cual puede afectar la estabilidad de modelos como la Regresión Logística.\n",
    "4. **Segmentación de Clientes:** Útil para validar si los clusters generados tienen una asociación real con variables demográficas.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0faa32c",
   "metadata": {},
   "source": [
    "# Residuos de Pearson: Análisis de Dependencia Local\n",
    "\n",
    "Mientras que la **V de Cramer** nos da un número único para la asociación global, los **Residuos de Pearson** nos permiten mirar \"dentro\" de la tabla de contingencia para entender qué categorías específicas están atrayendo o repeliendo a otras.\n",
    "\n",
    "### 1. Definición Matemática\n",
    "El residuo de Pearson para una celda específica en una tabla de contingencia se define como:\n",
    "\n",
    "$$r_{i,j} = \\frac{O_{i,j} - E_{i,j}}{\\sqrt{E_{i,j}}}$$\n",
    "\n",
    "Donde:\n",
    "* $O_{i,j}$: Frecuencia **Observada** (lo que realmente ocurrió).\n",
    "* $E_{i,j}$: Frecuencia **Esperada** bajo la hipótesis de independencia ($E = \\frac{\\text{total fila} \\times \\text{total columna}}{\\text{total gran}}$).\n",
    "\n",
    "**Nota:** El estadístico $\\chi^2$ es simplemente la suma de los cuadrados de estos residuos: $\\chi^2 = \\sum r_{i,j}^2$.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. ¿Cómo saber si las variables van juntas, separadas o no hay patrón?\n",
    "\n",
    "En estadística, para determinar la dirección y fuerza de la relación entre categorías específicas, utilizamos la **Escala de los Residuos Estandarizados**.\n",
    "\n",
    "| Valor del Residuo ($r$) | Interpretación del Patrón | Significado |\n",
    "| :--- | :--- | :--- |\n",
    "| **$r > 2$** | **Atracción (Van juntas)** | Hay significativamente más casos de los esperados. Existe una asociación positiva fuerte entre esas categorías. |\n",
    "| **$r < -2$** | **Repulsión (Van separadas)** | Hay significativamente menos casos de los esperados. Las categorías tienden a excluirse mutuamente. |\n",
    "| **$-2 < r < 2$** | **Independencia (No hay patrón)** | La diferencia se debe al azar. Las variables no muestran una relación especial en esa combinación. |\n",
    "\n",
    "> **Regla de oro:** Si un residuo supera **|1.96|** (aproximadamente 2), se considera estadísticamente significativo con un nivel de confianza del 95%.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Profundización en los Patrones\n",
    "\n",
    "#### A. El Patrón de \"Atracción\" ($r >> 0$)\n",
    "Si analizamos \"Fumadores\" vs \"Cáncer de Pulmón\" y el residuo es $+4.5$, significa que la combinación ocurre mucho más de lo que dictaría el azar. En Machine Learning, esto indica una **regla de asociación fuerte**.\n",
    "\n",
    "#### B. El Patrón de \"Repulsión\" ($r << 0$)\n",
    "Si analizamos \"Uso de Cinturón\" vs \"Muerte en Accidente\" y el residuo es $-5.2$, indica que la presencia de una categoría \"repele\" la otra. Es un factor protector o inversamente relacionado.\n",
    "\n",
    "#### C. Ausencia de Patrón ($r \\approx 0$)\n",
    "Si el residuo es $0.1$, las variables son independientes en ese punto. No importa qué valor tome la variable A, no afecta la probabilidad de la variable B.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Uso en Machine Learning y Ciencia de Datos\n",
    "\n",
    "1. **Ingeniería de Características (Grouping):** Si dos categorías de una variable tienen residuos similares respecto al target, pueden agruparse en una sola categoría para simplificar el modelo.\n",
    "2. **Análisis de Errores:** Permite ver en qué segmentos específicos nuestro modelo está fallando al predecir clases categóricas.\n",
    "3. **Visualización con Mosaic Plots:** Los residuos de Pearson se utilizan para dar color a los gráficos de mosaico. Las celdas azules indican atracción y las rojas repulsión.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
