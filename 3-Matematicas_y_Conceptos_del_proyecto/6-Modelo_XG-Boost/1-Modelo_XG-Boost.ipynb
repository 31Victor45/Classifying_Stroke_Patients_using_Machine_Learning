{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80a95305",
   "metadata": {},
   "source": [
    "# XGBoost: El Rey de los Algoritmos de Boosting\n",
    "\n",
    "XGBoost no es solo un modelo, es una biblioteca diseñada para ser eficiente, flexible y portátil. Pertenece a la familia del **Gradient Boosting**, lo que significa que construye árboles de forma secuencial, donde cada nuevo árbol intenta corregir los errores de los anteriores.\n",
    "\n",
    "## 1. La Función Objetivo (La Matemática del Éxito)\n",
    "\n",
    "A diferencia de otros algoritmos, XGBoost define una **función objetivo formal** que intenta minimizar en cada paso. Esta función tiene dos partes:\n",
    "\n",
    "$$\\text{Obj}(\\theta) = L(\\theta) + \\Omega(\\theta)$$\n",
    "\n",
    "1. **$L(\\theta)$ (Pérdida de Entrenamiento):** Mide qué tan bien el modelo predice los datos (ej. Error Cuadrático Medio o Log-Loss).\n",
    "2. **$\\Omega(\\theta)$ (Regularización):** Esta es la clave de XGBoost. Castiga la complejidad de los árboles para evitar el **overfitting**.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. El Corazón Matemático: La Expansión de Taylor\n",
    "\n",
    "XGBoost utiliza la **Serie de Taylor** de segundo orden para optimizar la función de pérdida de forma rápida. Para cada iteración $t$, buscamos un árbol $f_t$ que minimice:\n",
    "\n",
    "$$\\text{Obj}^{(t)} \\approx \\sum_{i=1}^{n} [l(y_i, \\hat{y}_i^{(t-1)}) + g_i f_t(x_i) + \\frac{1}{2} h_i f_t^2(x_i)] + \\Omega(f_t)$$\n",
    "\n",
    "Donde:\n",
    "* **$g_i$ (Gradient):** La primera derivada de la función de pérdida. Nos dice la dirección del error.\n",
    "* **$h_i$ (Hessian):** La segunda derivada. Nos dice la \"curvatura\" del error, permitiendo pasos más precisos.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. ¿Cómo decide dónde dividir un nodo? (Gain)\n",
    "\n",
    "XGBoost no usa Gini o Entropía como un árbol normal. Usa una métrica llamada **Ganancia (Gain)** basada en la estructura del árbol. La fórmula para decidir un \"split\" es:\n",
    "\n",
    "$$\\text{Gain} = \\frac{1}{2} \\left[ \\frac{G_L^2}{H_L + \\lambda} + \\frac{G_R^2}{H_R + \\lambda} - \\frac{(G_L + G_R)^2}{H_L + H_R + \\lambda} \\right] - \\gamma$$\n",
    "\n",
    "* **$G_L, G_R$:** Suma de gradientes en el lado izquierdo y derecho.\n",
    "* **$H_L, H_R$:** Suma de Hessianos.\n",
    "* **$\\lambda$:** Regularización L2 (evita pesos muy grandes).\n",
    "* **$\\gamma$:** El \"peaje\" por crear una nueva rama. Si la ganancia no supera $\\gamma$, el árbol no crece (Poda automática)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d494d3d",
   "metadata": {},
   "source": [
    "## 4. Características Únicas de su Aprendizaje\n",
    "\n",
    "### A. Manejo de Valores Faltantes (Sparsity-aware Split Finding)\n",
    "XGBoost aprende automáticamente una **dirección por defecto** para cada nodo. Si llega un dato nulo, el modelo ya sabe si mandarlo a la izquierda o derecha basándose en qué dirección redujo más la pérdida durante el entrenamiento.\n",
    "\n",
    "### B. Aprendizaje Paralelo y Distribuido\n",
    "Aunque los árboles se construyen en secuencia, la búsqueda de los puntos de corte (splits) se realiza de forma **paralela** utilizando todos los núcleos de tu CPU, lo que lo hace increíblemente rápido.\n",
    "\n",
    "### C. Tree Pruning (Poda Profunda)\n",
    "A diferencia de otros modelos que dejan de crecer cuando la ganancia es negativa, XGBoost crece hasta la profundidad máxima (`max_depth`) y luego empieza a **podar hacia atrás** (post-pruning) eliminando ramas que no aportan una ganancia positiva real."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dabccb5e",
   "metadata": {},
   "source": [
    "### 5. Resumen Didáctico: La Analogía del Escultor\n",
    "Imagina que quieres esculpir una estatua perfecta de piedra:\n",
    "\n",
    "**Árbol de Decisión normal:** Intenta esculpir la estatua de un solo golpe. A veces le sale bien, pero suele romper la piedra (Overfitting).\n",
    "\n",
    "Gradient Boosting (XGBoost):\n",
    "\n",
    "* El **primer escultor** quita los trozos grandes de piedra (primer árbol).\n",
    "* El **segundo escultor** mira los errores del primero y lija las partes que quedaron toscas (segundo árbol).\n",
    "* El **tercer escultor** se enfoca solo en los detalles milimétricos (tercer árbol).\n",
    "* El **Hessiano ($h_i$)** es como la lupa del escultor, que le dice cuánta presión debe aplicar según la dureza de la piedra en esa zona específica."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
