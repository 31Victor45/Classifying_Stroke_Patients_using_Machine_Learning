{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa0eae52",
   "metadata": {},
   "source": [
    "# Métricas de Evaluación en Clasificación Binaria\n",
    "\n",
    "En la clasificación binaria, donde se predice la pertenencia a una de dos clases (e.g., $Y \\in \\{0, 1\\}$, como 'Benigno' o 'Maligno'), las métricas se centran en evaluar la capacidad del modelo para distinguir correctamente ambas clases y manejar el potencial **desbalance de clases**.\n",
    "\n",
    "## 1. Matriz de Confusión\n",
    "\n",
    "La **Matriz de Confusión** es la base de la mayoría de las métricas de clasificación. Muestra el número de predicciones correctas e incorrectas para cada clase.\n",
    "\n",
    "| | Predicción Positiva ($\\hat{Y}=1$) | Predicción Negativa ($\\hat{Y}=0$) |\n",
    "| :--- | :--- | :--- |\n",
    "| **Real Positivo ($Y=1$)** | **VP** (Verdaderos Positivos) | **FN** (Falsos Negativos) |\n",
    "| **Real Negativo ($Y=0$)** | **FP** (Falsos Positivos) | **VN** (Verdaderos Negativos) |\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Métricas Basadas en Frecuencia (Umbral Fijo)\n",
    "\n",
    "Estas métricas se calculan directamente a partir de la matriz de confusión.\n",
    "\n",
    "### A. Accuracy (Precisión General)\n",
    "Mide la proporción de predicciones correctas sobre el total de predicciones. Es la métrica más simple, pero es engañosa en casos de desbalance de clases.\n",
    "\n",
    "$$\\mathbf{Accuracy} = \\frac{VP + VN}{VP + VN + FP + FN}$$\n",
    "\n",
    "### B. Recall o Sensibilidad (Tasa de Verdaderos Positivos, TPR)\n",
    "Mide la capacidad del modelo para encontrar todas las instancias positivas (evitando Falsos Negativos). Es crucial cuando no se quiere fallar en detectar la clase minoritaria (e.g., cáncer o fraude).\n",
    "\n",
    "$$\\mathbf{Recall} = \\frac{VP}{VP + FN}$$\n",
    "\n",
    "### C. Precision (Precisión Predictiva Positiva, PPV)\n",
    "Mide la proporción de identificaciones positivas que fueron realmente correctas. Es crucial cuando el costo de un Falso Positivo es alto.\n",
    "\n",
    "$$\\mathbf{Precision} = \\frac{VP}{VP + FP}$$\n",
    "\n",
    "### D. F1-Score\n",
    "Es la media armónica de la Precisión y el Recall. Es una métrica de equilibrio que penaliza fuertemente a los modelos que tienen un rendimiento pobre en cualquiera de los dos (Precisión o Recall), siendo ideal para conjuntos desbalanceados.\n",
    "\n",
    "$$\\mathbf{F1} = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}} = \\frac{2 \\cdot VP}{2 \\cdot VP + FP + FN}$$\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Métricas Basadas en Probabilidad (Independientes del Umbral)\n",
    "\n",
    "Estas métricas evalúan la calidad de las probabilidades de clase que produce el modelo, no solo el resultado final de la votación binaria.\n",
    "\n",
    "### E. Área bajo la Curva ROC (AUC-ROC)\n",
    "Mide la capacidad del modelo para distinguir entre clases en todos los posibles umbrales de clasificación. Representa la probabilidad de que el clasificador ordene aleatoriamente una instancia positiva por encima de una instancia negativa.\n",
    "\n",
    "* **Fórmula Conceptual:** El AUC es el área bajo la curva que traza la **Tasa de Verdaderos Positivos (TPR)** contra la **Tasa de Falsos Positivos (FPR)**.\n",
    "\n",
    "$$\\mathbf{AUC} = \\int_{0}^{1} \\text{TPR}(t) \\, d(\\text{FPR}(t))$$\n",
    "\n",
    "Donde:\n",
    "$$\\text{TPR} = \\text{Recall} \\quad \\text{y} \\quad \\text{FPR} = \\frac{FP}{FP + VN}$$\n",
    "\n",
    "* **Interpretación:** Un valor de $\\mathbf{0.5}$ indica un rendimiento aleatorio; un valor de $\\mathbf{1.0}$ indica un clasificador perfecto. El AUC es la métrica de elección para evaluar la capacidad de **discriminación** en datasets desbalanceados.\n",
    "\n",
    "### F. Log Loss (Pérdida Logarítmica o Entropía Cruzada)\n",
    "Mide la penalización basada en la confianza de la predicción de probabilidad. Penaliza al modelo cuando está muy seguro de una predicción que resulta ser incorrecta.\n",
    "\n",
    "$$\\mathbf{Log Loss} = - \\frac{1}{N} \\sum_{i=1}^{N} \\left[ y_i \\log(\\hat{p}_i) + (1 - y_i) \\log(1 - \\hat{p}_i) \\right]$$\n",
    "\n",
    "Donde:\n",
    "* $N$ es el número de instancias.\n",
    "* $y_i$ es la etiqueta real (0 o 1).\n",
    "* $\\hat{p}_i$ es la probabilidad predicha de que $y_i=1$.\n",
    "\n",
    "* **Uso:** Es ideal para evaluar la **calibración** (calidad de las probabilidades) del modelo de Bosques Aleatorios."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
