{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4de295de",
   "metadata": {},
   "source": [
    "# Hiperparámetros Clave en Modelos de Ensamble Basados en Árboles\n",
    "\n",
    "Estos hiperparámetros controlan la complejidad y el tamaño tanto de los árboles individuales como del ensamble general (Bosque Aleatorio, GBT, etc.). Su ajuste es fundamental para controlar el *trade-off* entre **sesgo (bias)** y **varianza**.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Parámetro del Ensamble\n",
    "\n",
    "Este parámetro controla la cantidad de modelos individuales que componen el ensamble.\n",
    "\n",
    "### A. `n_estimators` (Número de Estimadores)\n",
    "Define el **número de árboles de decisión** que se construirán en el bosque o en la secuencia de *boosting*.\n",
    "\n",
    "* **Efecto:** Un valor más alto generalmente reduce la varianza del modelo, mejorando la robustez.\n",
    "* **Trade-off:** Aumenta el tiempo de entrenamiento y predicción linealmente.\n",
    "* **Fórmula Conceptual:** La predicción final es un promedio (Regresión) o una votación (Clasificación) de los $K$ árboles.\n",
    "    $$\\text{Predicción Final} = \\frac{1}{K} \\sum_{k=1}^{K} \\hat{y}_k \\quad \\text{donde } K = \\text{n\\_estimators}$$\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Parámetros de Control de la Complejidad Individual del Árbol\n",
    "\n",
    "Estos parámetros limitan el crecimiento de cada árbol para prevenir el sobreajuste.\n",
    "\n",
    "### B. `max_depth` (Profundidad Máxima)\n",
    "Define la **máxima longitud del camino** desde el nodo raíz hasta el nodo terminal de un árbol.\n",
    "\n",
    "* **Efecto:** Es la restricción más directa sobre la complejidad del árbol. Si es muy bajo, el modelo puede subajustar (*underfit*). Si es ilimitado, cada árbol tiende a sobreajustar a los datos de entrenamiento.\n",
    "* **Optimización:** Se busca el punto donde el modelo aprende patrones sin memorizar ruido.\n",
    "\n",
    "### C. `min_samples_split` (Mínimo de Muestras para Dividir)\n",
    "Define el **número mínimo de observaciones** que debe tener un nodo para que se considere realizar una nueva división.\n",
    "\n",
    "* **Efecto:** Un valor alto impide que el árbol realice divisiones muy específicas en nodos con pocas muestras, promoviendo árboles más genéricos y reduciendo el sobreajuste.\n",
    "* **Unidad:** Puede ser un valor entero (número absoluto de muestras) o un valor decimal (proporción de la muestra total).\n",
    "\n",
    "### D. `min_samples_leaf` (Mínimo de Muestras en la Hoja)\n",
    "Define el **número mínimo de observaciones** que debe tener un nodo terminal (hoja) después de que se realiza una división.\n",
    "\n",
    "* **Efecto:** Es una restricción más estricta que `min_samples_split`. Garantiza que cada predicción de hoja se base en una cantidad suficiente de datos, lo que hace que las predicciones sean más estables y menos sensibles al ruido de los datos de entrenamiento.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Parámetro de Aleatoriedad y Diversidad (Específico de Random Forest)\n",
    "\n",
    "### E. `max_features` (Número Máximo de Características)\n",
    "Define el **número máximo de variables predictoras** que se considerarán aleatoriamente en cada nodo al buscar la mejor división.\n",
    "\n",
    "* **Efecto:** Este es el motor principal de la **diversidad** en Random Forest. Un valor bajo (e.g., $\\sqrt{\\text{total features}}$) aumenta la descorrelación entre los árboles, reduciendo la varianza general del ensamble.\n",
    "* **Fórmulas Comunes (Reglas de Oro):**\n",
    "    * **Clasificación:** $\\text{max\\_features} = \\sqrt{p}$\n",
    "    * **Regresión:** $\\text{max\\_features} = p/3$\n",
    "    $$\\text{Diversidad} \\propto 1 / \\text{Correlación}_{\\text{árboles}}$$"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
