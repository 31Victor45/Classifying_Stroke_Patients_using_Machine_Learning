{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e28d1e66",
   "metadata": {},
   "source": [
    "# Algoritmo de Bosques Aleatorios para Clasificación Binaria (Random Forest Classifier)\n",
    "\n",
    "La **Clasificación con Bosques Aleatorios** es un método de aprendizaje supervisado de tipo **Ensamble** que, al igual que su contraparte de regresión, utiliza múltiples Árboles de Decisión durante el entrenamiento. En el contexto de la clasificación binaria (donde la variable objetivo $Y$ tiene solo dos posibles resultados, $\\mathbf{0}$ o $\\mathbf{1}$), el resultado final se determina por **votación mayoritaria** de los árboles.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Fundamentos: Bagging y Aleatoriedad\n",
    "\n",
    "Random Forest se basa en dos pilares para reducir la varianza y el sobreajuste: **Bagging** y la **Aleatoriedad de Características**.\n",
    "\n",
    "### A. Bagging (Bootstrap Aggregating)\n",
    "Se generan $K$ subconjuntos de datos (muestras bootstrap) muestreando el conjunto de datos original con reemplazo. Cada árbol $k$ del bosque se entrena con su propia muestra bootstrap.\n",
    "\n",
    "### B. Aleatoriedad de Características\n",
    "En cada nodo de cada árbol, solo se considera un subconjunto aleatorio de $m$ características ($m \\leq p$) al buscar la mejor división. Esto garantiza que los árboles sean diversos y descorrelacionados.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Construcción del Árbol de Clasificación\n",
    "\n",
    "Cada árbol individual en el bosque es un **Árbol de Decisión de Clasificación**. El criterio para decidir la mejor división en cada nodo es el que maximiza la **Pureza** de los nodos hijos o, equivalentemente, el que reduce más la **Impureza**. Los criterios más comunes son la **Ganancia de Información (basada en Entropía)** o el **Índice de Gini**.\n",
    "\n",
    "### A. Índice de Gini (Criterio de Impureza Común)\n",
    "El Índice de Gini mide la probabilidad de clasificar erróneamente un elemento si se selecciona al azar. Una impureza de 0.0 indica que el nodo es puro (todos los elementos pertenecen a la misma clase).\n",
    "\n",
    "$$\\mathbf{Gini}(t) = 1 - \\sum_{j=1}^{C} (p_j)^2$$\n",
    "\n",
    "Donde:\n",
    "* $t$ es el nodo que se evalúa.\n",
    "* $C$ es el número de clases (en clasificación binaria, $C=2$).\n",
    "* $p_j$ es la proporción de instancias de la Clase $j$ en el nodo $t$.\n",
    "\n",
    "### B. Criterio de Decisión\n",
    "El algoritmo selecciona la característica y el punto de división que maximizan la **Reducción de Impureza** (o Ganancia de Gini).\n",
    "\n",
    "$$\\text{Ganancia Gini} = \\mathbf{Gini}_{\\text{padre}} - \\sum_{hijo \\in \\{\\text{izq, der}\\}} w_{hijo} \\cdot \\mathbf{Gini}_{\\text{hijo}}$$\n",
    "\n",
    "Donde $w_{hijo}$ es la proporción de instancias que caen en el nodo hijo.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Predicción Final (Votación Mayoritaria)\n",
    "\n",
    "Para hacer una predicción para una nueva instancia $\\mathbf{x}$, la instancia es pasada a través de todos los $K$ árboles entrenados. Cada árbol genera una predicción de clase ($\\hat{y}_k \\in \\{0, 1\\}$).\n",
    "\n",
    "### A. Votación de la Clase\n",
    "El resultado final del Bosque Aleatorio es la clase que recibe la **mayoría de los votos** de todos los árboles.\n",
    "\n",
    "$$\\mathbf{\\hat{Y}}_{\\text{RF}}(\\mathbf{x}) = \\text{Moda} \\{\\hat{y}_1(\\mathbf{x}), \\hat{y}_2(\\mathbf{x}), \\dots, \\hat{y}_K(\\mathbf{x})\\}$$\n",
    "\n",
    "### B. Probabilidades de Clase\n",
    "Además de la clase, Random Forest puede generar una probabilidad de predicción al contar la proporción de árboles que votaron por una clase específica.\n",
    "\n",
    "$$\\mathbf{P}(Y=1|\\mathbf{x}) = \\frac{\\text{Número de árboles que votaron por la Clase } 1}{K}$$"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
