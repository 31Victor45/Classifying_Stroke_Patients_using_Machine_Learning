{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94d42584",
   "metadata": {},
   "source": [
    "# ¿Por qué transformar todas las variables y cómo interpretar el Skewness?\n",
    "\n",
    "Cuando trabajamos con variables que tienen diferentes niveles de asimetría (*skewness*), surge la duda de si debemos ser selectivos o tratar a todas por igual. Aquí analizamos la lógica detrás de transformar incluso aquello que no parece \"tan mal\".\n",
    "\n",
    "## 1. ¿Por qué transformar las 3 variables (aunque una sea \"casi\" normal)?\n",
    "\n",
    "Aunque una variable tenga un sesgo leve, aplicarle la misma transformación (como Box-Cox o Yeo-Johnson) que a las demás es una buena práctica por tres razones:\n",
    "\n",
    "1. **Homogeneidad de la Escala Matemática:** Al aplicar la misma familia de transformaciones, los coeficientes del modelo se vuelven más comparables. Si transformas dos y dejas una \"cruda\", el modelo está lidiando con naturalezas matemáticas distintas.\n",
    "2. **Estabilización de la Varianza:** El sesgo suele venir acompañado de varianza no constante. Incluso un sesgo leve puede ocultar una dispersión que afecta a modelos sensibles como la Regresión Logística.\n",
    "3. **El Parámetro $\\lambda$ es Inteligente:** La transformación Box-Cox busca el $\\lambda$ óptimo. \n",
    "   * Si una variable ya es normal, el algoritmo encontrará un $\\lambda \\approx 1$.\n",
    "   * Aplicar Box-Cox con $\\lambda = 1$ es equivalente a **no hacer nada** (deja los datos casi iguales).\n",
    "   * **Conclusión:** No le haces daño a la variable \"sana\", y aseguras que pase por el mismo proceso de refinamiento.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. El Rango Mágico: $\\pm 0.5$\n",
    "\n",
    "En estadística, el coeficiente de asimetría (*skewness*) mide qué tan alejada está la distribución de la simetría perfecta (donde el valor es 0)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2067bd",
   "metadata": {},
   "source": [
    "### ¿Por qué $\\pm 0.5$ se considera aceptable?\n",
    "\n",
    "El criterio estándar de los científicos de datos es el siguiente:\n",
    "* **Entre -0.5 y 0.5:** La distribución es **simétrica** (o casi simétrica). Se considera \"Normal\" para propósitos prácticos de ML.\n",
    "* **Entre -1 y -0.5 o 0.5 y 1:** Sesgo moderado.\n",
    "* **Menor a -1 o mayor a 1:** Sesgo altamente pronunciado (Transformación necesaria).\n",
    "\n",
    "**¿Por qué aceptamos 0.5?**\n",
    "Porque los modelos de Machine Learning son \"robustos\". No necesitan una perfección matemática absoluta; necesitan que los datos no tengan \"colas\" tan largas que confundan al algoritmo. Un valor de 0.4, por ejemplo, indica que la media, mediana y moda están lo suficientemente cerca como para que el modelo no se sesgue.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Ejemplo Visual de Transformación y Skewness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8645d479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skewness Original:\n",
      "V1    1.939337\n",
      "V2    1.492072\n",
      "V3    0.112344\n",
      "dtype: float64\n",
      "\n",
      "Skewness Tras Transformación:\n",
      "V1    0.072382\n",
      "V2   -0.001983\n",
      "V3   -0.002116\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import skew, boxcox\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Creamos 3 variables con diferente sesgo\n",
    "n = 1000\n",
    "v1 = np.random.exponential(scale=2, size=n)      # Muy sesgada\n",
    "v2 = np.random.gamma(shape=2, scale=2, size=n)   # Sesgo moderado\n",
    "v3 = np.random.normal(loc=10, scale=1, size=n)   # Casi normal\n",
    "\n",
    "data = pd.DataFrame({'V1': v1, 'V2': v2, 'V3': v3})\n",
    "\n",
    "print(\"Skewness Original:\")\n",
    "print(data.skew())\n",
    "\n",
    "# Aplicamos Box-Cox a las 3\n",
    "data_tf = pd.DataFrame()\n",
    "lambdas = {}\n",
    "\n",
    "for col in data.columns:\n",
    "    # Sumamos constante para asegurar valores positivos\n",
    "    data_tf[col], lambdas[col] = boxcox(data[col] + 1)\n",
    "\n",
    "print(\"\\nSkewness Tras Transformación:\")\n",
    "print(data_tf.skew())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c8b577",
   "metadata": {},
   "source": [
    "## 4. Conclusión Didáctica: El Umbral de Tolerancia\n",
    "\n",
    "¿Qué pasa si después de Box-Cox tu variable tiene un skew de **0.4**? \n",
    "**¡Es un éxito!** * No busques el 0.0 perfecto. Intentar forzar un 0.0 absoluto puede llevar a un \"sobre-ajuste\" de los datos o a crear distribuciones artificiales que pierden sentido físico.\n",
    "* El rango **$\\pm 0.5$** es el \"punto dulce\" donde la matemática del modelo (como los residuos en una Regresión) se comporta de manera estable y predecible.\n",
    "\n",
    "---\n",
    "\n",
    "### Resumen para tu flujo de trabajo:\n",
    "1. **Transforma las 3:** Permite que el algoritmo decida el $\\lambda$ (si es 1, no afectará a la variable normal).\n",
    "2. **Evalúa el resultado:** Si el skew final está en el rango $[-0.5, 0.5]$, tu preprocesamiento ha terminado y tus datos están listos para el entrenamiento."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
