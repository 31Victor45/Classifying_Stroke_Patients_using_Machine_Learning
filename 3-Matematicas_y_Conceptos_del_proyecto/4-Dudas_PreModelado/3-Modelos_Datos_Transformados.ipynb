{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "752f8e33",
   "metadata": {},
   "source": [
    "# ¿Por qué usar datos transformados (Box-Cox + Escalado) para todos los modelos?\n",
    "\n",
    "Cuando competimos o comparamos modelos como **Regresión Logística**, **Random Forest** y **XGBoost**, surge la duda: si los árboles no lo necesitan, ¿para qué hacerlo? Aquí te doy las razones de peso.\n",
    "\n",
    "## 1. El \"Denominador Común\" (Comparabilidad)\n",
    "\n",
    "Si quieres comparar el desempeño de tres modelos, lo ideal es que todos partan de la **misma base de información**. \n",
    "\n",
    "* **Regresión Logística:** Es \"obligatorio\" escalar y normalizar. Si no lo haces, el modelo podría no converger o dar pesos ($\\beta$) erróneos.\n",
    "* **Modelos de Árboles (RF y XGBoost):** Aunque son inmunes a la escala, **no son inmunes a la distribución**. Si aplicas Box-Cox, estás reduciendo la varianza extrema y el sesgo, lo que a menudo ayuda a los árboles a encontrar \"cortes\" (splits) más limpios y profundos con menos esfuerzo.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Beneficios Específicos por Modelo\n",
    "\n",
    "| Modelo | ¿Por qué le sirve el Escalado? | ¿Por qué le sirve Box-Cox? |\n",
    "| :--- | :--- | :--- |\n",
    "| **Regresión Logística** | **Vital.** Evita que variables grandes dominen el modelo y acelera el gradiente descendiente. | **Muy útil.** Ayuda a que la relación sea más lineal y cumpla los supuestos estadísticos. |\n",
    "| **XGBoost** | **Opcional.** No cambia la lógica, pero puede ayudar ligeramente en la velocidad de cálculo interna. | **Útil.** Al normalizar la variable objetivo o los predictores, el cálculo del gradiente es más estable. |\n",
    "| **Random Forest** | **Neutral.** No lo necesita para funcionar, pero tampoco le perjudica. | **Positivo.** Al reducir el sesgo, las particiones de los nodos son más equilibradas. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727a97d1",
   "metadata": {},
   "source": [
    "## 3. La Razón \"Oculta\": La Estabilidad del Entrenamiento\n",
    "\n",
    "### A. Prevención de Valores Atípicos (Outliers)\n",
    "Box-Cox \"comprime\" los valores extremadamente lejanos. En **XGBoost**, que es un modelo que aprende de los errores (residuos) de los árboles anteriores, un outlier muy agresivo puede hacer que el modelo se enfoque demasiado en ese punto (Overfitting). Al transformar, suavizas ese riesgo.\n",
    "\n",
    "### B. Facilidad en el Deployment (Puesta en producción)\n",
    "Es mucho más sencillo mantener un solo **Pipeline** de preprocesamiento para todos tus experimentos que tener tres tuberías de datos distintas para cada modelo. \n",
    "\n",
    "---\n",
    "\n",
    "## 4. Conclusión Didáctica: El Principio de la \"Mejor Versión\"\n",
    "\n",
    "Imagina que vas a una carrera con tres autos distintos.\n",
    "1. Uno necesita gasolina premium (Regresión Logística).\n",
    "2. Los otros dos funcionan con gasolina común (Árboles).\n",
    "\n",
    "¿Es mala idea ponerle premium a los tres? **No.** A los que usan común no les hará daño (y quizás corran un poco mejor), y te aseguras de que el auto que la necesita rinda al máximo. \n",
    "\n",
    "**En resumen:** Transformar para todos asegura que la Regresión Logística compita en igualdad de condiciones y que los modelos de ensamble tengan datos más \"limpios\" y manejables."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
