{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68e38973",
   "metadata": {},
   "source": [
    "# Test de Shapiro-Wilk: La Prueba de Normalidad\n",
    "\n",
    "El test de Shapiro-Wilk es una prueba estadística que se utiliza para contrastar si un conjunto de datos proviene de una **distribución normal**. Es considerada la prueba más potente y confiable, especialmente para muestras pequeñas y medianas (hasta 5,000 datos).\n",
    "\n",
    "### 1. El Planteamiento de Hipótesis\n",
    "Como todo test estadístico, Shapiro-Wilk trabaja con dos hipótesis:\n",
    "\n",
    "* **Hipótesis Nula ($H_0$):** Los datos siguen una distribución normal.\n",
    "* **Hipótesis Alternativa ($H_1$):** Los datos **NO** siguen una distribución normal.\n",
    "\n",
    "### 2. ¿Cómo se interpreta? (El P-valor)\n",
    "Aquí es donde muchos se confunden, ¡pero es muy simple! Solo debes mirar el **p-value**:\n",
    "\n",
    "* **Si p-valor > 0.05:** No tenemos evidencia suficiente para rechazar $H_0$. Por lo tanto, **asumimos que nuestros datos son Normales**.\n",
    "* **Si p-valor < 0.05:** Rechazamos $H_0$. Esto significa que nuestros datos tienen un sesgo o una curtosis que los aleja de la normalidad (**No son Normales**).\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### 3. ¿Por qué se usa en Machine Learning?\n",
    "\n",
    "Muchos algoritmos y técnicas de ML son \"especialistas\" en datos normales. Usamos Shapiro-Wilk para decidir qué camino tomar en nuestro flujo de trabajo:\n",
    "\n",
    "#### A. Selección de Algoritmos\n",
    "* Si tus variables son normales, algoritmos como la **Regresión Lineal**, **Regresión Logística** o el **Análisis de Discriminante Lineal (LDA)** suelen funcionar con su máxima eficiencia.\n",
    "* Si NO son normales, quizás prefieras algoritmos no paramétricos o basados en árboles (**Random Forest, Gradient Boosting**).\n",
    "\n",
    "#### B. Elección de Pruebas Estadísticas\n",
    "* **Datos Normales:** Puedes usar el Test de T-Student o ANOVA.\n",
    "* **Datos No Normales:** Debes usar pruebas como Mann-Whitney o Kruskal-Wallis.\n",
    "\n",
    "#### C. Preprocesamiento (Escalado)\n",
    "* Si los datos pasan el test de Shapiro-Wilk, el **StandardScaler** (Z-score) es ideal.\n",
    "* Si los datos fallan el test, un **MinMaxScaler** o un **RobustScaler** suele ser más adecuado para no distorsionar la distribución.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Limitaciones Importantes\n",
    "Aunque es el mejor test, tiene un \"truco\":\n",
    "1.  **Sensibilidad al tamaño:** En datasets muy grandes (ej. 1 millón de filas), casi cualquier desviación mínima hará que el test diga que \"No es normal\" ($p < 0.05$). En esos casos, es mejor complementar con gráficos como el **Q-Q Plot**.\n",
    "2.  **Muestreo:** Si tienes muchísimos datos, se recomienda tomar una muestra aleatoria de 500 o 1,000 registros para aplicar Shapiro-Wilk.\n",
    "\n",
    "\n",
    "\n",
    "### Ejemplo rápido en interpretación:\n",
    "> \"Hice el test a mi columna 'Edad'. El p-valor salió 0.0002. Como es menor a 0.05, mi columna 'Edad' no es normal y debería considerar una transformación logarítmica o usar un modelo robusto.\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
